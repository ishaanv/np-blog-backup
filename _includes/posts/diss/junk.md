<hr />
<h1 id="conclusion">Conclusion</h1>
The desire that cities are currently showing to promote archi-tourism only serves to highlight the level of media coverage that unconventional architecture attracts. However the current climate surrounding digital design is notable now not for what the media covers, but for what it ignores. Nobody is writing articles about new buildings being drawn up using AutoCAD LT[^20] to produce neat drawings. The use of CAD for the production of drawings is now considered universal, using traditional methods is now deemed, as it is in so many industries, either a rejection of modern methods for a cultural or intellectual reason, or a lack of professionalism in not having the correct tools for the job. In the not too distant future, the use of three dimensional design tools such as Archicad[^21] or Architectural Desktop[^22] will be ubiquitous and therefore also deemed unworthy of media attention. One can only hope that eventually the cutting edge tools and methods will loose their newness and be adopted by more conservative practices. The competitive advantage of being able to produce finished designs in a few days by using parametric design programs, as demonstrated by the stadia designed by Arup Sport, will force the industry to pay attention and begin to adopt the methods.

There is a general fear of algorithms amongst architects who’s predominantly ‘artistic’ nature disposes them to be distrustful of technical things, but when one parallels parametric design with the technology of rendering then it becomes less daunting in the long term. Ten years ago rendering was a highly mathematical and very much programming based, requiring an in depth knowledge of the processes involved. These days there is an enormous level of automation already built into the software, making rendering extremely simple. Parametric design is intrinsically more complicated than rendering, as individual rule sets need to be formulated whereas, in rendering, once the way in which light behaves is codified it is just a matter of refining it, but as software advances and more tools are imbedded in the interface, it will become more instinctive. Structures is easily refined back to maths, maths to programming, programming to tools, these tools allow designers to work with the real thing, in real time.

The design possibilities offered by combining digital and analogue computing provide fascinating possibilities for producing beautiful elegant forms. Both specifically designed analogue computing devices (as used by <span class="small-caps">NOX</span>) and the study of biological entities (Emergence and Design Group) give an insight to material performance that was lacking throughout most of the modern movement. The important thing to consider when employing these methods is that in the production of biomimetic designs, the cell or plant structure is essentially diagrammatic and the designer must abstract the structural rule set and apply it in a recontextualised way. We should not be building towers that look like termite mounds or daisies, but rather that we use the structural and organisational techniques intrinsic to their implication.

<figure>
<img src="{{ site.baseurl }}/assets/15/diss/CausticsPool.jpg" alt="CausticsPool" >
<figcaption>
A rendered scene with animated water that refracts light. Ten years ago this would have been unthinkable.
</figcaption>
</figure>

Robert Aish of Bentley Systems draws a graph of the idealised design process, a straight line from inception to completion. He then overlays the realistic path taken by a design as its characteristics are modified, advanced, regressed and this path is anything but linear. The further along this path the design is, the less likely it is that problems will be solved in the ideal way, a damage control patch is the most likely eventuality. For instance, if the design has a complicated curved space-frame incorporating the entrance and the space-frame’s geometry is manually calculated, then the need for a minor change such as repositioning the entrance, would, in conventional workflow, most likely be ignored. The entrance would be left where it is and the landscaping changed, producing a less than ideal design due to time and budgetary constraints. However, if the space-frame was designed using a parametric algorithm, then moving the entrance would just require a subtle adjustment to the model and to click ‘recalculate’, and minutes later the change would be made. With parametric design, fairly large changes can be made right up to the last minute before the tool paths are sent to the fabricators.

<figure>
<img src="{{ site.baseurl }}/assets/15/diss/"> TODO
<figcaption>
One of Gaudi’s hanging chain models
</figcaption>
</figure>

<figure>
<img src="{{ site.baseurl }}/assets/15/diss/spaceframe.jpg">
</figure>

The notion that digital computers are making the inconceivable possible is somewhat inaccurate. The majority of complex forms created today have been possible (albeit in less advanced materials) for centuries. The Sagrada Família is made in both an incredibly complex and advanced form, but it has been constructed on a time scale and budget inconceivable with modern financial constraints. Modern computational methods make constructions like this infinitely easier: Greg Lynn’s definition of the difference between architecture and sculpture suggests architecture is defined by its use of components to create a whole[^33] and under this definition the Colossus in Rhodes was the first biomimetic piece of architecture. It was built over twelve years using hundreds of hand beaten bronze panels to form a gigantic form of the god Helios to guard the harbour. By using modern digital design and fabrication methods, the labour intensive techniques used to form the beaten panels, could be completely automated using an algorithm to cut up the whole figure into components and then add the connection details. The panels could be identified with barcodes to define their precise location and orientation to enable completely seamless onsite assembly. The forms can be outputted to a CNC machine to mill the moulds out of foam for an epoxy carbon Kevlar panel. Using modern digital workflow the colossus could be constructed in under a year.

As digital design becomes more widespread the current method of architectural education may need to change too, not necessarily to produce programmers or mathematicians at the expense of design skills, but nobody starts their design career with aspirations of spending their time doing door schedules, and if computers can be used to do the menial work then designers who can use them effectively will be freed up to spend more time designing. Indeed already there is a skills gap in digital design practitioners. Instead of the conventional idea of computer virtuosos being young upstarts, the main protagonists in the digital design field are all over 40 (Novak – 48, Lynn – 41, Gehry – 76.) They were all involved in computing relatively close to its inception when programming and understanding the workings of the computer were essential to operating the machine. This ‘insider’ knowledge has allowed them to manipulate the computer to do what they want as opposed to being constrained by the software developer’s built in settings. Since the introduction of Microsoft Windows and other GUI operating systems the level of abstraction and digital handholding has increased at the expense of an awareness of the underlying principles. As the total number of computer users has increased the number of people actually skilled enough to push software to its limits seems to be proportionately decreasing. This has no detrimental effect on the majority of computer users, but for innovation to occur there needs to be a certain level of understanding. This is beginning to be addressed through architectural education, but more and more digital design is likely to form a central part of an architecture syllabus. With practitioners such as Karl Chu, Greg Lynn and Lars Spuybroek being invited to teach and continue their research at universities, the education that some students receive is at the cutting edge, preparing them to take over as the next generation of innovators.

<figure>
<img src="{{ site.baseurl }}/assets/15/diss/" >TODO
<figcaption>
Dali : Collossus
</figcaption>
</figure>

The concept of architect as tool builder, one who designs the rules that the system adheres to and then lets the system (structure) grow and evolve into a finished form, is difficult to grasp for a generation weaned on the slightly megalomaniac paradigm of architect as deity, controlling all things, and generally compromising complexity to achieve a human level of involvement. Many fear that relinquishing control to the apparently serendipitous results of the computer will diminish the quality of the resultant design. One main concern is that as people publish new tools there will be an ensuing wave of buildings which have clearly used that tool in their design, just as student renderings often use identifiably standard texturing. This prejudice is justifiable but needs to be eradicated through a combination of education in ‘tool building’ and through general pressure from the design community as a whole to retain originality and reject cookie cutter projects. The worry that designs will loose their human touch and fall prey to ruthless computerised efficiency is, however, unfounded. The simple answer is that if the resultant solution is unsatisfactory, it can be ignored and the process started again, or one can return along the path taken to get to the present position, change a variable and re-run, exactly as if it were a manually conceived design. For architecture to continually evolve there needs to be a continual shedding of approaches. Marcus Novak makes the case succinctly when he asserts that the frank Gehry approach of manual modelling with subsequent computer construction was a fitting end to the twentieth century, but it has no place in the twenty first[^34]

The one thing that makes digital design stand out from what has preceded it is it’s ability to deal with complexity. Everything that can be done with a computer is also possible without it - a computer is just a box with wires that does maths, and as such can be replicated, given sufficient time with a pen and paper. Time is the crucial factor however, because as Gaudí proved with the Sagrada Família, there isn’t enough of it to do the calculations in a human lifetime (he died with less than half the church completed). The concept of finite element analysis was invented before computers, but was never pursued because it was too complex to do any sort of meaningful analysis manually. Calculating the algorithms that govern an emergent system, or the polynomial and simultaneous equations that govern a surface in a parametric design rely on the power of computers to do the calculations in a split second. This acceleration of complexity drives new forms and new organisational and structural systems to become a reality.


The area of design that has been spawned by computers is designing architecture in cyberspace. If we are to continue our integration with the internet and mobile communications at our current rate of consumption then it is conceivable that we will soon begin to ‘inhabit’ the internet rather than merely viewing it as we do now. As every generation of architects struggles with the question “…but is it architecture?” ours has the digital frontier to cross, and the associated removal of restrictions will doubtless produce some fascinating work as well as some interesting questions about how we conceive and inhabit our comparatively restrictive physical world.

<figure>
<img src="{{ site.baseurl }}/assets/15/diss/" >TODO
<figcaption>
Alien Bio
</figcaption>
</figure>

